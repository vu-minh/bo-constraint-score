%!TEX spellcheck

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Setups}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Datasets}

Six diverse datasets (of grayscale and color images, text and gene expressions) are used to show the application of our method to the real world dataset. (See Table.~\ref{tbl:dataset} for some samples of each dataset.)

\emph{DIGITS} is a subset of the optical recognition of handwritten digits dataset~\cite{kaynak1995methods} of 8x8 grayscale images.
\emph{COIL20} \cite{nene1996} is a dataset of 32x32 grayscale images of 20 rotated objects.
\emph{FASHION\_1K} contains 1000 grayscale images of size 28x28, sampled from Fashion-MNIST\cite{xiao2017/online} clothing dataset.
The grayscale images from three above datasets are normalized and used directly in the DR methods.

\emph{FASHION\_MOBILENET} dataset contains samples of 7 most numerous classes from a subset of Fashion Product images dataset~\cite{fashionproduct}.
The MobileNet\cite{howard2017mobilenets} with pre-trained weights from ImageNet is used for feature extraction, a transfer learning technique that uses the representation of the learned network (trained on a large-scale image classification task) to extract meaningful features for new samples.
The last fully connected layer of the network is replaced by a global average pooling layer\cite[Sec.3.2]{lin2013network} to obtain the flattened output vector of 1280 dimensions.
To speed up the DR methods, PCA is then applied to take only 75 features.

\emph{5NEWS} dataset contains the text of 5 groups selected from the 20 Newsgroups dataset which are converted to a matrix of token counts via Term Frequency Inverse Document Frequency (TF-IDF) method.
The count vectors are then fed into Latent Dirichlet Allocation (LDA)~\cite{blei2003latent} model to extract 15 hidden topics, which are 15 features used for DR methods.

The open \emph{NEURON\_1K}~\cite{neuron1k} dataset contains 1301 brain cells from an E18 Mouse, processed and provided by 10X Genomics, a company who provides chromium single cell gene expression solution and releases several public genetic datasets.\footnote{https://www.10xgenomics.com/resources/datasets/, the datasets are licensed under the Creative Commons Attribution license.}
The processed data have 10 PCA features and labeled in 6 classes found by a graph-based clustering method.


\begin{table*}[width=\textwidth,cols=6,pos=h]
\caption{Description of six selected datasets.}\label{tbl:dataset}
\begin{tabular}{m{3cm} m{4.8cm} m{8cm}}
\toprule
Dataset name & Samples & Description \\
\midrule

\emph{COIL20}
    & \includegraphics[width=\linewidth]{COIL20_samples}
    & 1440 grayscale images of size 32x32, belonging to 20 classes.
    The raw images of 1024 dimensions are used directly for the DR methods.\\

\emph{DIGITS}
    & \includegraphics[width=\linewidth]{DIGITS_samples}
    & 1797 grayscale images of size 8x8 of 10 digits.
    The raw images of 64 dimensions are used directly for the DR methods.\\

\emph{FASHION\_1K}
    & \includegraphics[width=\linewidth]{FASHION1000_samples}
    & 1000 grayscale images of size 28x28 of 10 classes, sampled from Fashion-MNIST dataset.
    The raw images of 768 dimensions are used directly for the DR methods.\\

\emph{FASHION\_MOBILENET}
    & \includegraphics[width=\linewidth]{FASHION_PRODUCT_samples}
    & 1494 color images of various sizes belonging to 7 classes
    (\path{'Bags', 'Bottomwear', 'Jewellery', 'Sandal', 'Shoes', 'Topwear', 'Watches'}),
    sampled from Fashion Product images dataset.\\

\emph{5NEWS}
    & \includegraphics[width=\linewidth]{20NEWS5_samples}
    & 5 groups of 2957 emails selected from 20Newsgroups dataset,
    including \path{'rec.autos', 'rec.sport.baseball','sci.crypt', 'sci.space', 'comp.sys.mac.hardware'}. \\

\emph{NEURON\_1K}
    &
    & 1301 brain cells from a combined cortex, hippo-campus and sub-ventricular zone of an E18 mouse. \\

\bottomrule
\end{tabular}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Constraint Generation}

The input for our proposed constraint preserving scores is ensemble of constraints in form of similar links and dissimilar links.
However, it is hard to control the quality of the constraints.
For example, a similar link between two different data points is not expected.
The number of constraints each type also affects the stability of the score.
In order to correctly evaluate the score function, we do not use the pairwise constraints directly, but use a small subset of instances with labels to generate the constraints instead.

Give a dataset of $C$ classes, for each class we take randomly $k$ labeled instances to generate the constraints.
Suppose that the number of instances in each class is larger than $k$ ($k$ is usually very small).
A similar link is formed by choosing a random pair from $k$ labeled instances of a class.
The number of all possible similar links is given by:
\begin{equation}\label{equ:n-sim-link}
n_{sim\_link} = C {k \choose 2} = \frac{1}{2} C k (k-1)
\end{equation}
A dissimilar link is formed by first choosing two different classes among $C$ classes (${C \choose 2}$ ways),
then choosing a pair of two instances from these two chosen classes ($k^2$ pairs).
The number of all possible dissimilar links is given by:
\begin{equation}\label{equ:n-dis-link}
n_{dis\_link} = {C \choose 2} k^2 = \frac{1}{2} C (C-1) k^2
\end{equation}

Intuitively, the more labeled points we had, the more constraints are generated and the more stable the $f_{score}$ is.
We will show in the next section the experimental setup for the proof of concept.
Indeed, with 10 labeled points for each of 10 classes of the DIGITS dataset for example, they are around 5.6\% of total labels but can generate 450 similar links and 4500 dissimilar links.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Concept}
We design the experiments to answer the research questions concerning to the stability (Section~\ref{sec:x}) and flexibility (Section~\ref{sec:z}) of the proposed score and the application of BayOpt to $f_{score}$ (Section~\ref{sec:y}).
As a proof of concept, we create a grid of hyperparameters for t-SNE and UMAP, and then calculate $f_{score}$ for each embedding corresponding to each combination.
The hyperparameters are sampled in natural logarithmic scale (log-scale) since we want to focus on the values which are not too large.
In the case of t-SNE, the grid is an integer vector of perplexity values from 2 to $N // 3$ in log-scale, in which $N$ is the number of instances of the dataset.
In the case of UMAP, the two dimensional grid is all combinations of its two hyperparameters.
The first dimension is an integer vector of $n\_neighbor$ values from 2 to $N // 3$ in log-scale.
The second dimension is a vector of 10 real values of $min\_dist$ from 0.001 to 1.0 in log-scale.
This grid helps us to analyze the behavior of the proposed $f_{score}$ w.r.t. different combinations of hyperparameters.
The $f_{score}$ values on this grid are also used as ground-truth score values to evaluate the prediction of BayOpt approach.
It is important to note that, the creation of this grid is computationally expensive and increases exponentially with the number of hyperparameters.
The grid is only used as proof of concept to evaluate the reliability of the BayOpt approach on the proposed score.
As shown in the next section, BayOpt approach solves the scalability problem efficiently, in which only dozens of evaluations are required to find the best hyperparameters for all experimented datasets.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Stability of Constraint Preserving Score}
The proposed constraint preserving score is a function of the input constraints $f_{score}(\mathcal{S}, \mathcal{D})$.
The first question is that, how does $f_{score}$ vary with respect to different number of labeled instances?
Moreover, with a fixed number of labeled instances, we can generate different set of pairwise constraints.
The second question is thus, with different set of constraints generated from the same fixed number of labeled instances, is $f_{score}$ stable?

Stability of the score: analyze Fig.~\ref{fig:score:stability:COIL20} and explain how BayOpt approach can take into account the variance (uncertainty) of the score.

\begin{figure*}[pos=h]
     \centering
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{COIL20_umap_scores}
         \caption{UMAP with COIL20}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{COIL20_tsne_scores}
         \caption{t-SNE with COIL20}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{COIL20_largevis_scores}
         \caption{LargeVis with COIL20}
     \end{subfigure}
     \caption{Stability of the constraint preserving scores with three methods UMAP, t-SNE and LargeVis for COIL20 dataset.}
     \label{fig:score:stability:COIL20}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application of BayOpt on $f_{score}$}
BayOpt is widely used for hyperparameter tuning problem when the target function is well-defined.
In supervised setting or in clustering problem where we can access the class labels, the target function is simply a metric which measures the quality of the prediction or clustering.
However in visualization problem, the target function for measuring the quality of the visualization is not well-defined yet.
Throughout experiments, we show that the proposed $f_{score}$ can work well under BayOpt framework as a flexible quality measurement.
Indeed, BayOpt is a general framework which can work with any complex target function like $AUC_{log}RNX$ score or BIC-based score function.
In practice, $AUC_{log}RNX$ does not always work well UMAP's embedding, especially in the case of tuning two UMAP's hyperparameters at the same time.
BIC-based score tiered 
$f_{score}$, $AUC_{log}RNX$ and BIC-based scores are compared in both tasks of tuning one hyperparameter for tSNE (Sec~\ref{sec:x}) and tuning two hyperparameters for UMAP (Sec~\ref{sec:y}).

\subsection{BayOpt for Tuning One Parameter of t-SNE}

\subsubsection*{BayOpt in Action}
See Fig.~\ref{fig:tsne:bo:all}.

\begin{figure*}[pos=h]
    \begin{subfigure}[b]{.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{DIGITS_tsne_bo.png}
        \caption{\emph{DIGITS}}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{COIL20_tsne_bo.png}
        \caption{\emph{COIL20}}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{NEURON_1K_tsne_bo.png}
        \caption{\emph{NEURON\_1K}}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{FASHION1000_tsne_bo.png}
        \caption{\emph{FASHION1000}}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{FASHION_MOBILENET_tsne_bo.png}
        \caption{\emph{FASHION\_MOBILENET}}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{20NEWS5_tsne_bo.png}
        \caption{\emph{5NEWS}}
    \end{subfigure}
    ~
    \caption{BayOpt in action for t-SNE}
    \label{fig:tsne:bo:all}
\end{figure*}


\subsubsection*{Compare $f_{score}$ with $AUC_{log}RNX$ and BIC}
See Fig.~\ref{fig:tsne:compare}.

\begin{figure*}[pos=h]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{DIGITS_tsne_compare_scores}
        \caption{DIGITS}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{COIL20_tsne_compare_scores}
        \caption{COIL20}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{NEURON_1K_tsne_compare_scores}
        \caption{NEURON\_1K}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{FASHION1000_tsne_compare_scores}
        \caption{FASHION1000}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{FASHION_MOBILENET_tsne_compare_scores}
        \caption{FASHION\_MOBILENET}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{20NEWS5_tsne_compare_scores}
        \caption{5NEWS}
    \end{subfigure}
    \caption{Comparing constraint score, $AUC_{log}RNX$ score and BIC score for the embeddings of tSNE.}
    \label{fig:tsne:compare}
\end{figure*}


\subsubsection*{Metamaps and Visualizations}
Metamap and sample visualizations for \emph{DIGITS} (Fig.~\ref{fig:tsne:meta:DIGITS}) and \emph{NEURON\_1K} (Fig.~\ref{fig:tsne:meta:NEURON1K}).

\begin{figure*}[pos=h]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{{DIGITS_tsne_metamap}.png}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{DIGITS_tsne_show}
    \end{subfigure}
    \caption{Metamap and sample visualizations for the selected parameters for \emph{DIGITS} dataset.}
    \label{fig:tsne:meta:DIGITS}
\end{figure*}

\begin{figure*}[pos=h]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{{NEURON_1K_tsne_metamap}.png}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{NEURON_1K_tsne_show}
    \end{subfigure}
    \caption{Metamap and sample visualizations for the selected parameters for \emph{NEURON\_1K} dataset.}
    \label{fig:tsne:meta:NEURON1K}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{BayOpt for Tuning Two Parameters of UMAP}

\subsubsection*{BayOpt in Action}
BayOpt for tuning two parameters of UMAP with \emph{COIL20} (Fig.~\ref{fig:bo:umap:COIL20}) and \emph{FASHION\_1K} (Fig.~\ref{fig:bo:umap:FASHION1000})

\begin{figure}[pos=h]
    \begin{subfigure}[b]{.85\linewidth}
        \includegraphics[width=\textwidth]{COIL20_umap_auc_rnx}
        \caption{$AUC_{log}RNX$}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.85\linewidth}
        \includegraphics[width=\textwidth]{COIL20_umap_qij_score}
        \caption{constraint score}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\textwidth]{COIL20_umap_predicted_score}
        \caption{BO predicted score}
    \end{subfigure}
    ~
    \caption{COIL20}
    \label{fig:bo:umap:COIL20}
\end{figure}

\begin{figure}[pos=h]
    \begin{subfigure}[b]{.85\linewidth}
        \includegraphics[width=\textwidth]{FASHION1000_umap_auc_rnx}
        \caption{$AUC_{log}RNX$}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.85\linewidth}
        \includegraphics[width=\textwidth]{FASHION1000_umap_qij_score}
        \caption{constraint score}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\textwidth]{FASHION1000_umap_predicted_score}
        \caption{BO predicted score}
    \end{subfigure}
    ~
    \caption{\emph{FASHION\_1K}}
    \label{fig:bo:umap:FASHION1K}
\end{figure}

\subsubsection*{Compare $f_{score}$ with $AUC_{log}RNX$ (default min\_dist 0.1)}
See annex 2.

\subsubsection*{Metamaps and Visualizations}

Metamap and sample visualizations for \emph{COIL20} (Fig.~\ref{fig:umap:meta:COIL20}) and \emph{FASHION\_1K} (Fig.~\ref{fig:umap:meta:FASHION1000}).

\begin{figure*}
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{COIL20_umap_metamap}.png}
        \caption{Metamap for UMAP embeddings of COIL20 dataset.}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{COIL20_umap_show}
        \caption{Sample vizs COIL20}
    \end{subfigure}
\end{figure*}


\begin{figure*}
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{FASHION1000_umap_metamap}.png}
        \caption{Metamap for UMAP embeddings of FASHION1000 dataset.}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{FASHION1000_umap_show}
        \caption{Sample vizs FASHION1000}
    \end{subfigure}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Flexibility of the Proposed Score}

t-SNE/LargeVis and UMAP are widely used since they have ability to preserve the neighborhood information, i.e. to make the data points in the same class close together.
In this way, the local structure of the data is highlighted.
But as pointed out in the article ``How to use t-sne effectively'' by \citet{wattenberg2016use}, the distance between clusters might not mean anything.
The hyperparameters of t-SNE and UMAP make these methods difficult to use correctly but also make them flexible.
These methods can produce different visualizations and reveal different hidden structures in the data.
To evaluate the embedding quality of such flexible methods, we need a flexible score.
The state of the art $AUC_{log}RNX$ metric or the BIC-based score has no flexibility to capture different visualization results.

$AUC_{log}RNX$ metric evaluates how the neighborhood information is preserved.
The BIC-based score evaluates the trade-off between KL loss and the value of perplexity in t-SNE.
In contrast, the $f_{score}$ evaluates how the constraints are preserved.

$f_{score}$ takes a set of constraints as input to find the best visualization which respects the constraints.
Naturally, the constraints generated from the class labels reflect the class-relationship between the data points in the same class.
% However, if we have another subset of \emph{`semantic'} labels

Demo score's flexibility for \emph{FASHION\_MOBILENET}(Fig.~\ref{fig:flex:fashionmobilenet}), \emph{5NEWS} (Fig.~\ref{fig:flex:5news}) and \emph{NEURON\_1K} (Fig.~\ref{fig:flex:neuron}).

\begin{figure*}[pos=h]
    \centering
    \includegraphics[width=\textwidth]{{FASHION_MOBILENET_score_flexibility}.png}
    \caption{Flexibility of $f_{score}$ for \emph{FASHION\_MOBILENET} dataset}
    \label{fig:flex:fashionmobilenet}
\end{figure*}

\begin{figure*}[pos=h]
    \centering
    \includegraphics[width=\textwidth]{{20NEWS5_score_flexibility}.png}
    \caption{Flexibility of $f_{score}$ for \emph{5NEWS} dataset}
    \label{fig:flex:5news}
\end{figure*}

\begin{figure*}[pos=h]
    \centering
    \includegraphics[width=\textwidth]{{NEURON_1K_score_flexibility}.png}
    \caption{Flexibility of $f_{score}$ for \emph{NEURON\_1K} dataset}
    \label{fig:flex:neuron}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion and Future Work}

\par
Discuss the variance of the proposed score (w.r.t to different set of constraints, different number of constraints).
Say, the constraint-based score is a stochastic function.
Say, how BayOpt can take into account the uncertainty (the variance of the score) to estimate the maximum of this stochastic function.


\par
The violated constraints in our methods correspond to the shortcoming of t-SNE in \cite{wattenberg2016use}.
t-SNE can not preserve the within-cluster distances and between-clusters distances.
[TODO: Add reproduced figures].
The $q_{ij}$-based score can help to understand the defective of the visualization.

\par
Add discussion for UMAP.

\par
Easily generate pairwise constraints from labels.
  Only need small amount of labeled points for each class to generate hundreds of constraints.
  The proposed method only need 200 constraints to work well.
  [TODO: Test with smaller number of constraints to see if it works. 200 constraints seem too much].

\par
Can replace the auto-generated constraints by the manual constraints of the real user.

\par
The user can interact directly in the loop of Bayesian Optimization method to select the next hyperparameter to discover.


\par
Both the constraint-based score and the BayOpt's internal steps are explainable even for the non-technical users. 

\vspace{8pt}
\par (4) Future work:

(a) User experiment:

+ Integrate the user's feedback in two stages of our workflow.
The users can select the pairwise constraints or label some points (used to generate the constraints) to build the score.
They can also manually select the next hyperparameters to evaluate in a customized interactive BayOpt framework.

+ Take the preference of the users on the presented visualizations to evaluate the quality of the visualization. We search for if the best visualization selected by the user corresponds to the result of our method.


(b) Integrate directly the pairwise constraints into the optimization process of BayOpt.
BayOpt is now used as a generic toolbox to find the extreme of a blackbox costly objective function.
Our idea is to use the pairwise constraint to modify the kernel in the covariance function of Gaussian Process model, which is the core element of BayOpt.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

\par (1) Repeat the problem of hyperparameter tuning for DR methods and our solution:

+ The proposed constraint-based score is independent to how the embedding is produced and can be used with any DR methods.
This score is built upon a limited number of constraints but can distinguish the visualizations preferring local structure and those preferring global structure.

+ A finding that Bayesian Optimization approach fits well in our problem.


\vspace{8pt}
\par (2) Summary the advantages of the two above elements

+ The constraint-based score agree the the well-known quality metric.

+ This score can be visually represented to explain the violated pairs.

+ By combining this score with BayOpt approach, we can tune many hyperparameters at the same time for many widely used DR methods like t-SNE or UMAP.

+ BayOpt takes into account the uncertainty in the score values and also explainable. We can observe the internal optimization step to answer the question: why to choose the next promising hyperparameters to try?