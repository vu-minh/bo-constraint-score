\section{Introduction}

\par
(1) Keep the session 1 (context, problematic).

\vspace{8pt} \par
(2) Add a small paragraph to introduce several visualization methods which are widely used in practice but hard to tune the (hyper)-parameters:
tSNE~\cite{maaten2008tsne}, LargeVis~\cite{tang2016visualizing}, UMAP~\cite{mcinnes2018umap}.

\vspace{8pt} \par
(3) Our solution:

+ Constraint preserving score to measure the similarity preserving in the visualization. (TODO need to find the goal of our score, and say why it is worth to measure the similarity in the viz).

+ Bayesian Optimization (BayOpt) approach~\cite{mockus1978application, brochu2010tutorial} for parameter tuning.

\vspace{8pt} \par
(4) Main contributions: 
TODO: Complete later.


\vspace{8pt}
\par
(5) The target audiences:

+ The end-users who want to apply the visualization methods to their own data without caring about the complex algorithms and parameters.
They can use our method as a blackbox parameter tuning toolbox with an additional price of providing the labels or a partial of the labels for the dataset.
(Refer to the section analyzing the impact of the number of constraints).

+ The experts who want to analze the impact of the parameters and to evaluate the quality of the visualization.
They can use our method as a transparent toolbox to understand the internal step in the optimization process thanks to BayOpt approach.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background and Related Work}

\subsection{Visualization Quality Metrics}
Keep the session 4.3

\subsection{Usage of Pairwise Constraints in Unsupervised Learning}
Keep the sections 3.2, 3.3.

\subsection{Choosing a Good Perplexity for t-SNE}
+ Automatic selection of perplexity for t-SNE, Cao and Wang~\cite{cao2017automatic}.

+ Analyze the pros and cons of this method.

+ This method can be referred as a baseline to compare with our approach.

\subsection{Automatic Parameter Tuning with Bayesian Optimization}

(1) Intro to BayOpt:

+ Refer to Mockus' serie of works on Bayesian method for seeking the extremum (Mockus1978, Mockus1982, Mockus1994).

+ Refer to the modern inroduction to BayOpt~\cite{brochu2010tutorial} with examples of its applications.


\vspace{8pt} \par
(2) Explain how BayOpt fit into our problem.

\vspace{8pt} \par
(3) Discuss: BayOpt can be applied to the BIC-based score~\cite{cao2017automatic}.
However this score has two disavantages:
(1) it is tied to the loss function of t-SNE.
(2) it works only with one parameter (perplexity of t-SNE), and thus can not be generalized for other DR methods.
Our proposed method can do better.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Constraint Preserving Score}

\subsection{Visual Definition of the Pairwise Constraints}
+ Keep the section 4.1

+ Explain that  we use only the auto-generated pairwise constraints.

\subsection{Quantify the Pairwise Constraints}
+ Keep the section 4.2

+ Add a sketch to illustrate the small distance of a similar link leads to large probability $q_{ij}$, thus a large score for must-link constraint. (the same for cannot-link constraint).

\subsection{Basic characteristics of Constraint Preserving Score}

+ Keep the section 6.2:

\hspace{10pt }- Must-link score agrees with CCA score (Fig~\ref{fig:sml}).
\begin{figure}
\centering
\includegraphics[scale=0.2]{sml_cca_50.pdf}
\caption{Must-link score agrees with CCA score.}\label{fig:sml}
\end{figure}

\hspace{10pt }- Cannot-link score agrees with BIC-based score (Fig.~\ref{fig:scl}).
\begin{figure}
\centering
\includegraphics[scale=0.2]{scl_bic_50.pdf}
\caption{Cannot-link score agrees with BIC-based score.}\label{fig:scl}
\end{figure}

\hspace{10pt }- ML+CL agrees with AUC\_RNX score (Fig.~\ref{fig:sall}).
\begin{figure}
\centering
\includegraphics[scale=0.2]{sall_auc_50.pdf}
\caption{ML+CL agrees with AUC\_RNX score.}\label{fig:sall}
\end{figure}

\vspace{8pt} \par
+ Variance / Stability of the score: analyze Fig.~\ref{fig:svar} and explain how BayOpt approach can take into account the variance (uncertainty) of the score.
\begin{figure}
\centering
\includegraphics[scale=0.2]{sall_perp30.pdf}
\caption{ML+CL agrees with AUC\_RNX score.}\label{fig:svar}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Constraint-based Score as Target in Bayesian Optimization Approach}

+ Explain the internal step in BayOpt.
Can make use some figures Fig.~\ref{fig:bayopt5}, Fig.\ref{fig:bayopt10}.

\begin{figure}
\centering
\includegraphics[scale=0.2]{{ucb_kappa5_constraint1.0_DIGITS_step5}.png}
\caption{BayOpt after 5 steps.}\label{fig:bayopt5}
\end{figure}


\begin{figure}
\centering
\includegraphics[scale=0.2]{{ucb_kappa5_constraint1.0_DIGITS_step10}.png}
\caption{BayOpt after 10 steps.}\label{fig:bayopt10}
\end{figure}

+ Explain how the utility function are constructed and optimized, and answer why optimize the utility function (surrogate function), we can optimize at the same time the target function (the constraint-based score function).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Results}

(1) Optimal parameters found by BayOpt w.r.t the constraint-based score:

+ BayOpt method for finding \verb|perplexity| param for t-SNE. (Case of 1 param to tune with t-SNE, Fig.~\ref{fig:tsne1}) (Done)
\begin{figure*}
\centering
\includegraphics[width=.6\textwidth]{{tSNE_ucb_kappa5_constraint1.0_DIGITS_step15}.png}
\caption{BayOpt with t-SNE with 1 param.}\label{fig:tsne1}
\end{figure*}


+ BayOpt for finding \verb|n_neighbors| param for UMAP. (Case of 1 param to tune with UMAP, Fig.~\ref{fig:umap1}) (Done)
\begin{figure*}
\centering
\includegraphics[width=.6\textwidth]{{UMAP_ucb_kappa5_constraint1.0_DIGITS_step15}.png}
\caption{BayOpt with UMAP with 1 param.}\label{fig:umap1}
\end{figure*}

+ BayOpt for finding \verb|n_neighbors| and \verb|min_dist| params for UMAP. (Case of 2 params to tune with UMAP, Fig.~\ref{fig:umap2}) (Plan to do)
\begin{figure*}
\centering
\includegraphics[width=0.6\textwidth]{{demo_bayopt_2params}.png}
\caption{TODO: Produce a figure similar to this one with 2 params of umap.}\label{fig:umap2}
\end{figure*}

\vspace{8pt} \par
(2) Visualization of the violated constraints. (Doing, a draft version looks like Fig.\ref{fig:viz-score})
\begin{figure}
\includegraphics[scale=0.3]{umap_vis_score.png}
\caption{TODO Visualize the constraint score (of the violated links).}\label{fig:viz-score}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

\par
Discuss the variance of the proposed score (w.r.t to different set of constraints, different number of constraitns).
Say, the constraint-based score is a stochastic function.
Say, how BayOpt can take into account the uncertainty (the variance of the score) to estimate the maximum of this stochastic function.


\par
The violated constraints in our methods correspond to the shortcomming of t-SNE in \cite{wattenberg2016use}.
t-SNE can not preserve the within-cluster distances and between-clusters distances.
[TODO: Add reproduced figures].
The $q_{ij}$-based score can help to understand the defective of the visualization.

\par
Easily generate pairwise constraints from labels.
  Only need small amount of labeled points for each class to generate hundreds of constraints.
  The proposed method only need 200 constraints to work well.

\par
Can replace the auto-generated constraints by the manual constraints of the real user.

\par
The user can interact directly in the loop of Bayesian Optimization method to select the next parameter to discover.


\par
Both the constraint-based score and the BayOpt's internal steps are explainable even for the non-technical users. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Future Work}

\par (1) Repeat the problem of parameter tuning for DR methods and our solution:

+ The proposed constraint-based score is indenpendent to the embedding and can be used with any DR methods.
This score is built upon a limited number of constraints but can distinguish the visualizations prefering local structure and those prefereing global structure.

+ A finding that Bayesian Optimization approach fits well in our problem.


\vspace{8pt}
\par (2) Summary the advantages of the two above elements

+ The constraint-based score agree the the well-known quality metric.

+ This score can be visually represented to explain the violated pairs.

+ By combining this score with BayOpt approach, we can tune many parameters at the same time for many widely used DR methods like t-SNE or UMAP.

+ BayOpt takes into account the uncertainty in the score values and also explainable. We can observe the internal optimization step to anwser the question: why to choose the next promissing parameters to try?

\vspace{8pt}
\par (3) Why we choose this approach of automatically tuning the parameter without modifying the choosen DR methods:

+ Easy to adapt to the existing DR methods. 

+ Towards AutoML (need citation) but can keep the explainability. Our method not only find the optimal visualization but also explain why it is.


\vspace{8pt}
\par (4) Future work:

+ Integrate the user's feedback in two stages of our workflow.
The users can select the pairwise constraints to build the score.
They can also manualy select the next parameters to evaluate in a customized interactive BayOpt framework.

+ Integrate directly the pairwise constraints into the optimization process of BayOpt.
BayOpt is now used as a generic toolbox to find the extreme of a blackbox costly objective function.
Our idea is to use the pairwise constraint to modify the kernel in the covariance function of Gaussian Process model, which is the core element of BayOpt.
